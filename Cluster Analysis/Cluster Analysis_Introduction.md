# **Cluster Analysis**

**Cluster Analysis** is a statistical technique used to group a set of similar objects into clusters such that:

* Objects within the same cluster are **highly similar** (high intra-cluster similarity), and
* Objects in different clusters are **dissimilar** (low inter-cluster similarity).

It is one of the primary methods in **unsupervised learning**, where the goal is to discover patterns or structures within unlabeled data.
<img width="312" height="162" alt="image" src="https://github.com/user-attachments/assets/83a17911-b932-480c-8e0d-f87b214fd380" />

## Dunn Index

Dunn index is the ratio of the minimum of inter-cluster distances and maximum of intracluster distances. 
The more the value of the Dunn index, the better the clusters will be.
![image](https://github.com/user-attachments/assets/c25f5868-6cd0-446a-beec-5ba8242e142d)

| Type                         | Description                                              | Goal               |
| ---------------------------- | -------------------------------------------------------- | ------------------ |
| **Intra-Cluster Similarity** | Similarity among objects **within** the same cluster.    | Should be **high** |
| **Inter-Cluster Similarity** | Similarity among objects **between** different clusters. | Should be **low**  |

Thus, a good clustering method aims to:

* Maximize intra-cluster similarity
* Minimize inter-cluster similarity

## Introduction

Clustering is a fundamental step in data mining, pattern recognition, and machine learning.
It helps identify hidden relationships in data by dividing it into meaningful subgroups (clusters) based on similarity.

* Each cluster represents a group of data points sharing common characteristics.
* Clustering can be used for exploratory data analysis, summarization, anomaly detection, and more.

Example: Grouping customers by purchasing behavior, documents by topic, or genes by expression patterns.

---


## **Advantages of Cluster Analysis**

* **No need for labeled data:** Works without predefined class labels.
* **Reveals hidden structures:** Finds natural groupings in data.
* **Flexible:** Can be applied to various data types and domains.
* **Preprocessing support:** Useful for summarizing data before classification or visualization.

---

## **Applications of Cluster Analysis**

* **Marketing:** Customer segmentation and target marketing
* **Biology:** Gene and protein classification
* **Image processing:** Object recognition and image compression
* **Social network analysis:** Community detection
* **Education:** Grouping students based on performance patterns
* **Healthcare:** Patient grouping for diagnosis and treatment plans

---

## **Types of Clustering Methods**

| Type                         | Description                                                                             | Examples                                          |
| ---------------------------- | --------------------------------------------------------------------------------------- | ------------------------------------------------- |
| **Hierarchical Clustering**  | Builds a hierarchy of clusters by successively merging or splitting clusters.           | Single linkage, Complete linkage, Average linkage |
| **Partitioning Clustering**  | Divides data into a fixed number of non-overlapping clusters.                           | K-Means, K-Medoids                                |
| **Density-Based Clustering** | Forms clusters based on regions of high density separated by low-density areas.         | DBSCAN, OPTICS                                    |
| **Model-Based Clustering**   | Assumes data is generated by a mixture of underlying probability distributions.         | Gaussian Mixture Models (GMM)                     |
| **Grid-Based Clustering**    | Divides data space into a finite number of cells and forms clusters from dense regions. | STING, CLIQUE                                     |

---

## **7. Hierarchical Clustering**

Hierarchical clustering builds a hierarchy (tree structure) of clusters called a **dendrogram**.
Two main approaches:

### **a. Agglomerative (Bottom-Up / AGNES)**

1. Start with each point as a separate cluster.
2. Iteratively merge the two most similar clusters.
3. Stop when all points belong to one cluster.

### **b. Divisive (Top-Down / DIANA)**

1. Start with all points in a single cluster.
2. Recursively split the clusters into smaller groups.
3. Stop when each point forms its own cluster or a desired number of clusters is reached.

---

## **8. Linkage Methods in Hierarchical Clustering**

| Method                                   | Description                                                                                              | Formula (if ( C_1 ) and ( C_2 ) are clusters)         |     |   |     |                                               |
| ---------------------------------------- | -------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- | --- | - | --- | --------------------------------------------- |
| **Single Linkage (Nearest Neighbor)**    | Distance between two clusters = **minimum distance** between any pair of points (one from each cluster). | ( D(C_1, C_2) = \min_{i \in C_1, j \in C_2} d(i, j) ) |     |   |     |                                               |
| **Complete Linkage (Farthest Neighbor)** | Distance between two clusters = **maximum distance** between any pair of points.                         | ( D(C_1, C_2) = \max_{i \in C_1, j \in C_2} d(i, j) ) |     |   |     |                                               |
| **Average Linkage**                      | Distance = **average distance** between all pairs of points from both clusters.                          | ( D(C_1, C_2) = \frac{1}{                             | C_1 |   | C_2 | } \sum_{i \in C_1} \sum_{j \in C_2} d(i, j) ) |

---

## **9. Common Distance Metrics**

| Metric                 | Formula                                   | Suitable for            |             |                                       |     |                            |   |   |     |                                |
| ---------------------- | ----------------------------------------- | ----------------------- | ----------- | ------------------------------------- | --- | -------------------------- | - | - | --- | ------------------------------ |
| **Euclidean Distance** | ( d(x, y) = \sqrt{\sum_i (x_i - y_i)^2} ) | Continuous numeric data |             |                                       |     |                            |   |   |     |                                |
| **Manhattan Distance** | ( d(x, y) = \sum_i                        | x_i - y_i               | )           | Grid or city-block data               |     |                            |   |   |     |                                |
| **Minkowski Distance** | ( d(x, y) = (\sum_i                       | x_i - y_i               | ^p)^{1/p} ) | General form of Euclidean & Manhattan |     |                            |   |   |     |                                |
| **Cosine Similarity**  | ( \text{sim}(x, y) = \frac{x \cdot y}{    |                         | x           |                                       | ,   |                            | y |   | } ) | Text and high-dimensional data |
| **Jaccard Similarity** | ( J = \frac{                              | A \cap B                | }{          | A \cup B                              | } ) | Categorical or binary data |   |   |     |                                |

---

## **10. Overlapping vs Non-Overlapping Clustering**

| Type                                                      | Description                                                                           | Example            |
| --------------------------------------------------------- | ------------------------------------------------------------------------------------- | ------------------ |
| **Overlapping Clustering**                                | An object can belong to **multiple clusters**. Useful when data has fuzzy boundaries. | Fuzzy C-Means      |
| **Non-Overlapping (Exclusive / Partitioning) Clustering** | Each object belongs to **exactly one cluster**.                                       | K-Means, K-Medoids |

---

## **11. Partitioning Clustering**

Partitioning methods divide a dataset into **k clusters**, where **each object belongs to exactly one cluster**.
The algorithm typically tries to **minimize the distance** between data points and their corresponding cluster centers.

### Example Algorithms:

* **K-Means Clustering**
* **K-Medoids (PAM – Partitioning Around Medoids)**

---

## **Advantages and Disadvantages of Hierarchical Clustering**

### **Advantages**

* No need to predefine the number of clusters.
* Produces an interpretable dendrogram.
* Can handle different types of distance metrics.
* Works well with small datasets.

### **Disadvantages**

* Computationally expensive (O(n³) time complexity).
* Not scalable for large datasets.
* Sensitive to noise and outliers.
* Once merged/split, clusters cannot be undone.

---

